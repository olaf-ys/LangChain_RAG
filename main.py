import streamlit as st
from PyPDF2 import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import FAISS
from langchain.tools.retriever import create_retriever_tool
from langchain.agents import AgentExecutor, create_tool_calling_agent
from langchain.chat_models import init_chat_model
from langchain_openai import OpenAIEmbeddings
import os
from dotenv import load_dotenv

load_dotenv(override=True)

# è®¾ç½®api key
api_key = os.getenv("OPENAI_API_KEY")
base_url = os.getenv("OPENAI_BASE_URL")

# åˆå§‹åŒ–embeddingæ¨¡å‹
embeddings = OpenAIEmbeddings(
    model="text-embedding-3-large",
    api_key=api_key,
    base_url=base_url
)

# å°†ä¸Šä¼ çš„pdfè§£ææˆæ–‡æœ¬
def pdf_read(pdf_doc):
    text = ""
    for pdf in pdf_doc:
        pdf_reader = PdfReader(pdf)
        for page in pdf_reader.pages:
            text += page.extract_text()
    return text

# å¯¹è§£æçš„æ–‡æœ¬åˆ†å—
def get_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    chunks = text_splitter.split_text(text)
    return chunks

# å¯¹åˆ†å—åçš„æ–‡æœ¬åˆ›å»ºembeddingsï¼Œç„¶åå‡ºå­˜åœ¨å‘é‡æ•°æ®åº“ä¸­
def vector_store(text_chunks):
    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
    vector_store.save_local("faiss_db")

def get_conversational_chain(tools, ques):

    # åˆå§‹åŒ–llm
    llm = init_chat_model(
        model="gpt-4o-mini", 
        model_provider="openai", 
        api_key=api_key, 
        base_url=base_url)
    
    # åˆ›å»ºæç¤ºè¯æ¨¡ç‰ˆ
    prompt = ChatPromptTemplate.from_messages([
        (
        "system",
        """ä½ æ˜¯AIåŠ©æ‰‹ï¼Œè¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼Œç¡®ä¿æä¾›æ‰€æœ‰ç»†èŠ‚ï¼Œå¦‚æœç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œè¯·è¯´"ç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­"ï¼Œä¸è¦æä¾›é”™è¯¯çš„ç­”æ¡ˆ""",
        ), # è®¾å®šAIåŠ©æ‰‹çš„è§’è‰²å’Œè¡Œä¸ºè§„åˆ™
        ("placeholder", "{chat_history}"), # ä¿å­˜å¯¹è¯å†å²è®°å½•çš„å ä½ç¬¦
        ("human", "{input}"), # ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
        ("placeholder", "{agent_scratchpad}"), # ä»£ç†çš„å·¥ä½œç©ºé—´ï¼Œç”¨äºå­˜å‚¨æ¨ç†è¿‡ç¨‹ 
    ])
    
    # é…ç½®å·¥å…·å’Œä»£ç†
    tool = [tools]
    agent = create_tool_calling_agent(llm, tool, prompt)
    agent_executor = AgentExecutor(agent=agent, tools=tool, verbose=True)
    
    # æ‰§è¡ŒæŸ¥è¯¢å¹¶æ˜¾ç¤ºç»“æœ
    response = agent_executor.invoke({"input": ques})
    print(response)
    st.write("ğŸ¤– å›ç­”: ", response['output'])

def check_database_exists():
    """æ£€æŸ¥FAISSæ•°æ®åº“æ˜¯å¦å­˜åœ¨"""
    return os.path.exists("faiss_db") and os.path.exists("faiss_db/index.faiss")

def user_input(user_question):
    # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
    if not check_database_exists():
        st.error("âŒ è¯·å…ˆä¸Šä¼ PDFæ–‡ä»¶å¹¶ç‚¹å‡»'Submit & Process'æŒ‰é’®æ¥å¤„ç†æ–‡æ¡£ï¼")
        st.info("ğŸ’¡ æ­¥éª¤ï¼š1ï¸âƒ£ ä¸Šä¼ PDF â†’ 2ï¸âƒ£ ç‚¹å‡»å¤„ç† â†’ 3ï¸âƒ£ å¼€å§‹æé—®")
        return
    
    try:
        # åŠ è½½FAISSæ•°æ®åº“
        new_db = FAISS.load_local("faiss_db", embeddings, allow_dangerous_deserialization=True)
        
        retriever = new_db.as_retriever()
        retrieval_chain = create_retriever_tool(retriever, "pdf_extractor", "This tool is to give answer to queries from the pdf")
        get_conversational_chain(retrieval_chain, user_question)
        
    except Exception as e:
        st.error(f"âŒ åŠ è½½æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}")
        st.info("è¯·é‡æ–°å¤„ç†PDFæ–‡ä»¶")

def main():
    st.set_page_config("æ£€ç´¢å¢å¼ºç”Ÿæˆ")
    st.header("æ£€ç´¢å¢å¼ºç”Ÿæˆ")
    
    # æ˜¾ç¤ºæ•°æ®åº“çŠ¶æ€
    col1, col2 = st.columns([3, 1])
    
    with col1:
        if check_database_exists():
            st.success("âœ… æ•°æ®åº“çŠ¶æ€ï¼šå·²å°±ç»ª")
        else:
            st.warning("âš ï¸ è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†PDFæ–‡ä»¶")
    
    with col2:
        if st.button("ğŸ—‘ï¸ æ¸…é™¤æ•°æ®åº“"):
            try:
                import shutil
                if os.path.exists("faiss_db"):
                    shutil.rmtree("faiss_db")
                st.success("æ•°æ®åº“å·²æ¸…é™¤")
                st.rerun()
            except Exception as e:
                st.error(f"æ¸…é™¤å¤±è´¥: {e}")

    # ç”¨æˆ·é—®é¢˜è¾“å…¥
    user_question = st.text_input("ğŸ’¬ è¯·è¾“å…¥é—®é¢˜", 
                                placeholder="ä¾‹å¦‚ï¼šè¿™ä¸ªæ–‡æ¡£çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ",
                                disabled=not check_database_exists())

    if user_question:
        if check_database_exists():
            with st.spinner("ğŸ¤” AIæ­£åœ¨åˆ†ææ–‡æ¡£..."):
                user_input(user_question)
        else:
            st.error("âŒ è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†PDFæ–‡ä»¶ï¼")

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.title("ğŸ“ æ–‡æ¡£ç®¡ç†")
        
        # æ˜¾ç¤ºå½“å‰çŠ¶æ€
        if check_database_exists():
            st.success("âœ… æ•°æ®åº“çŠ¶æ€ï¼šå·²å°±ç»ª")
        else:
            st.info("ğŸ“ çŠ¶æ€ï¼šç­‰å¾…ä¸Šä¼ PDF")
        
        st.markdown("---")
        
        # æ–‡ä»¶ä¸Šä¼ 
        pdf_doc = st.file_uploader(
            "ğŸ“ ä¸Šä¼ PDFæ–‡ä»¶", 
            accept_multiple_files=True,
            type=['pdf'],
            help="æ”¯æŒä¸Šä¼ å¤šä¸ªPDFæ–‡ä»¶"
        )
        
        if pdf_doc:
            st.info(f"ğŸ“„ å·²é€‰æ‹© {len(pdf_doc)} ä¸ªæ–‡ä»¶")
            for i, pdf in enumerate(pdf_doc, 1):
                st.write(f"{i}. {pdf.name}")
        
        # å¤„ç†æŒ‰é’®
        process_button = st.button(
            "ğŸš€ æäº¤å¹¶å¤„ç†", 
            disabled=not pdf_doc,
            use_container_width=True
        )
        
        if process_button:
            if pdf_doc:
                with st.spinner("ğŸ“Š æ­£åœ¨å¤„ç†PDFæ–‡ä»¶..."):
                    try:
                        # è¯»å–PDFå†…å®¹
                        raw_text = pdf_read(pdf_doc)
                        
                        if not raw_text.strip():
                            st.error("âŒ æ— æ³•ä»PDFä¸­æå–æ–‡æœ¬ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ")
                            return
                        
                        # åˆ†å‰²æ–‡æœ¬
                        text_chunks = get_chunks(raw_text)
                        st.info(f"ğŸ“ æ–‡æœ¬å·²åˆ†å‰²ä¸º {len(text_chunks)} ä¸ªç‰‡æ®µ")
                        
                        # åˆ›å»ºå‘é‡æ•°æ®åº“
                        vector_store(text_chunks)
                        
                        st.success("âœ… PDFå¤„ç†å®Œæˆï¼ç°åœ¨å¯ä»¥å¼€å§‹æé—®äº†")
                        st.balloons()
                        st.rerun()
                        
                    except Exception as e:
                        st.error(f"âŒ å¤„ç†PDFæ—¶å‡ºé”™: {str(e)}")
            else:
                st.warning("âš ï¸ è¯·å…ˆé€‰æ‹©PDFæ–‡ä»¶")
        
        # ä½¿ç”¨è¯´æ˜
        with st.expander("ğŸ’¡ ä½¿ç”¨è¯´æ˜"):
            st.markdown("""
            **æ­¥éª¤ï¼š**
            1. ğŸ“ ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªPDFæ–‡ä»¶
            2. ğŸš€ ç‚¹å‡»"Submit & Process"å¤„ç†æ–‡æ¡£
            3. ğŸ’¬ åœ¨ä¸»é¡µé¢è¾“å…¥æ‚¨çš„é—®é¢˜
            4. ğŸ¤– AIå°†åŸºäºPDFå†…å®¹å›ç­”é—®é¢˜
            
            **æç¤ºï¼š**
            - æ”¯æŒå¤šä¸ªPDFæ–‡ä»¶åŒæ—¶ä¸Šä¼ 
            - å¤„ç†å¤§æ–‡ä»¶å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´
            - å¯ä»¥éšæ—¶æ¸…é™¤æ•°æ®åº“é‡æ–°å¼€å§‹
            """)

if __name__ == "__main__":
    main()
